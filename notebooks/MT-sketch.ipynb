{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "A sketch of the [machine translation tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) from the [PyTorch Tutorials](https://pytorch.org/tutorials/index.html).\n",
    "\n",
    "It's the closest analogy to automated document conversion, so this is an attempt to study its internals.\n",
    "\n",
    "## Model\n",
    "\n",
    "This is a sequence-to-sequence (seq2seq) model, consisting of a pair of [recurrent neural networks (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network), the encoder and decoder. \n",
    "\n",
    "The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "![diagram](images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This is the first half of the RNN seq2seq network, aka [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf).\n",
    "\n",
    "For every input (word token, in this example) the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
    "\n",
    "![encoder](images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # using a multi-layer gated recurrent unit (GRU) \n",
    "        # as an improvement over long short-term memory (LSTM) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        embedded = self.embedding(input_layer).view(1, 1, -1)\n",
    "        output_layer, hidden_layer = self.gru(embedded, hidden_layer)\n",
    "        return output_layer, hidden_layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The is the second half of the model, another RNN that takes the encoder's output vectors as its input, and outputs a sequence of words to create the translation:\n",
    "\n",
    "![decoder](images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        output_layer = F.relu(self.embedding(input).view(1, 1, -1))\n",
    "        output_layer, hidden_layer = self.gru(output_layer, hidden_layer)\n",
    "        output_layer = self.softmax(self.out(output_layer[0]))\n",
    "        return output_layer, hidden_layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "\n",
    "Without this, the context vector which is passed betweeen the encoder and decoder carries the burden of encoding the entire sentence.\n",
    "\n",
    "The attention RNN allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs.\n",
    "\n",
    "![diagram](images/attention-decoder.png)\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer, `attn`, using the decoder's input and hidden state as inputs.\n",
    "\n",
    "Rather than using [bucketing](https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing) for handling inputs of variable length in the training data, this example chooses a maximum sentence length that gets applied to all inputs: sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
    "\n",
    "![diagram](images/attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout, max_length):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer, encoder_output):\n",
    "        embedded = self.dropout(self.embedding(input_layer).view(1, 1, -1))\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden_layer[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_output.unsqueeze(0))\n",
    "        output_layer = self.attention_combine(torch.cat((embedded[0], attention_applied[0]), 1)).unsqueeze(0)\n",
    "        output_layer, hidden_layer = self.gru(F.relu(output_layer), hidden_layer)\n",
    "        output_layer = F.log_softmax(self.out(output_layer[0]), dim=1)\n",
    "        return output_layer, hidden_layer, attention_weights\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, enc_optimizer, dec_optimizer, crit, max_len):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_len, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        encoder_outputs[i] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for i in range(target_length):\n",
    "        if random.random() < teacher_forcing_ratio:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += crit(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            loss += crit(decoder_output, target_tensor[i])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def in_minutes(seconds):\n",
    "    minutes = math.floor(seconds / 60)\n",
    "    seconds -= minutes * 60\n",
    "    return '%dm %ds' % (minutes, seconds)\n",
    "\n",
    "def since(prior, percent):\n",
    "    seconds = time.time() - prior\n",
    "    gap = (seconds / percent) - seconds\n",
    "    return '%s (- %s)' % (in_minutes(seconds), in_minutes(gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def show_plot(data):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensors(encoder, decoder, training_tensors, max_len, print_every, plot_every, learning_rate):\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    enc_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    dec_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    iterations = len(training_tensors)\n",
    "    for i in range(1, iterations+1):\n",
    "        training_pair = training_tensors[i-1]\n",
    "        input_tensor  = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, enc_optimizer, dec_optimizer, criterion, max_len)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (since(start, i/iterations), i, i/iterations * 100, print_loss_avg))\n",
    "            \n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    show_plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Using the tab-delimited examples from [Anki](https://www.manythings.org/anki/) to see if it's possible to produce a simple English to Japanese translation service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: parsing Japanese text in python\n",
    "\n",
    "This proved to be surprisingly complicated. \n",
    "\n",
    "[KyTea](http://www.phontron.com/kytea/) would not install until going down to version `0.3.2`, and even then would not run properly.\n",
    "\n",
    "[This guide](images/japanese-spacy-and-mecab.pdf) (archived from the [original](https://www.dampfkraft.com/nlp/japanese-spacy-and-mecab.html)) was helpful, but I needed a few more tweaks to make it work:\n",
    "\n",
    "* `sudo ldconfig` gets around the `error while loading shared libraries: libmecab.so.2` building `unidic-mecab-2.1.2_src` (hat tip to [tatsuyaoiw](http://tatsuyaoiw.hatenablog.com/entry/20120414/1334397985))\n",
    "* edited `/usr/local/etc/mecabrc` so that `dicdir` is defined as `dicdir = /usr/local/lib/mecab/dic/unidic`\n",
    "* after `pip install mecab-python3==0.7` and `pip install spacy` I also needed `pip install fugashi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-16 17:23:57--  https://www.manythings.org/anki/jpn-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3033::6818:6dc4, ...\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2127866 (2.0M) [application/zip]\n",
      "Saving to: ‘/tmp/jpn-eng.zip’\n",
      "\n",
      "jpn-eng.zip         100%[===================>]   2.03M  5.68MB/s    in 0.4s    \n",
      "\n",
      "2020-05-16 17:23:57 (5.68 MB/s) - ‘/tmp/jpn-eng.zip’ saved [2127866/2127866]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P /tmp https://www.manythings.org/anki/jpn-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/jpn-eng.zip\n",
      "  inflating: /tmp/jpn.txt            \n",
      "  inflating: /tmp/_about.txt         \n"
     ]
    }
   ],
   "source": [
    "!unzip -d /tmp /tmp/jpn-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, parser):\n",
    "        self.name = name\n",
    "        self.parser = parser\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in self.parser(sentence):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indices_from_sentence(self, sentence):\n",
    "        return [self.word2index[word] for word in self.parser(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Japanese language parser\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "ja = spacy.blank('ja')\n",
    "\n",
    "def parse_japanese(sentence):\n",
    "    return [word.text for word in ja(sentence)]\n",
    "\n",
    "\"\"\"\n",
    "English can be more complicated than this, \n",
    "but this suffices for this simple model\n",
    "\"\"\"\n",
    "\n",
    "def parse_english(sentence):\n",
    "    return sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_language_pairs(input_file='/tmp/jpn.txt'):\n",
    "    pairs = [] # list of (eng, jpn) pairs\n",
    "    for line in open(input_file, encoding='utf-8').read().splitlines():\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) > 1:\n",
    "            pairs.append((parts[0].strip(), parts[1].strip()))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 10\n",
    "\n",
    "def filter_by_word_length(pair):\n",
    "    \"\"\"\n",
    "    Since the AttentionDecoderRNN is based on a max_length,\n",
    "    this function provides a filter based on that parameter \n",
    "    \"\"\"\n",
    "    return len(pair) == 2 \\\n",
    "      and len(parse_english(pair[0])) < MAX_WORD_LENGTH \\\n",
    "      and len(parse_japanese(pair[1])) < MAX_WORD_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [pair for pair in parse_language_pairs() if filter_by_word_length(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24641 samples, 12320 training, 6160 validation, 6161 testing\n"
     ]
    }
   ],
   "source": [
    "# define the training, validation, testing data sets as 50 : 25 : 25\n",
    "sample_size = len(samples)\n",
    "training = samples[0:sample_size // 2]\n",
    "validation = samples[sample_size // 2:(sample_size // 2)+(sample_size // 4)]\n",
    "testing = samples[(sample_size // 2)+(sample_size // 4):]\n",
    "print(\"{} samples, {} training, {} validation, {} testing\"\n",
    "      .format(sample_size, len(training), len(validation), len(testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(parsed_pairs):\n",
    "    \"\"\"\n",
    "    Given a list of (eng, jpn) pairs, produce \n",
    "    a tuple (Lang(eng), Lang(jpn)) of the data\n",
    "    \"\"\"\n",
    "    eng = Lang('eng', parse_english)\n",
    "    jpn = Lang('jpn', parse_japanese)\n",
    "    for pair in parsed_pairs:\n",
    "        eng.add_sentence(pair[0])\n",
    "        jpn.add_sentence(pair[1])\n",
    "    return eng, jpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_training, jpn_training = prepare_sample(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5214, 4620)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_training.n_words, jpn_training.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting training data to [tensors](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor(lang, sentence):\n",
    "    indicies = lang.indices_from_sentence(sentence)\n",
    "    indicies.append(EOS_token)\n",
    "    return torch.tensor(indicies, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensors = [\n",
    "    (generate_tensor(eng_training, pair[0]), \n",
    "     generate_tensor(jpn_training, pair[1])) \n",
    "    for pair in training\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 27s (- 16m 32s) (1000 8%) 4.1927\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(eng_training.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttentionDecoderRNN(hidden_size, jpn_training.n_words, 0.1, MAX_WORD_LENGTH).to(device)\n",
    "\n",
    "train_tensors(encoder, attn_decoder, training_tensors, MAX_WORD_LENGTH, 1000, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
