{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "A sketch of the [machine translation tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) from the [PyTorch Tutorials](https://pytorch.org/tutorials/index.html).\n",
    "\n",
    "It's the closest analogy to automated document conversion, so this is an attempt to study its internals.\n",
    "\n",
    "## Model\n",
    "\n",
    "This is a sequence-to-sequence (seq2seq) model, consisting of a pair of [recurrent neural networks (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network), the encoder and decoder. \n",
    "\n",
    "The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "![diagram](images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This is the first half of the RNN seq2seq network, aka [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf).\n",
    "\n",
    "For every input (word token, in this example) the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
    "\n",
    "![encoder](images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # using a multi-layer gated recurrent unit (GRU) \n",
    "        # as an improvement over long short-term memory (LSTM) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        embedded = self.embedding(input_layer).view(1, 1, -1)\n",
    "        output_layer, hidden_layer = self.gru(embedded, hidden_layer)\n",
    "        return output_layer, hidden_layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The is the second half of the model, another RNN that takes the encoder's output vectors as its input, and outputs a sequence of words to create the translation:\n",
    "\n",
    "![decoder](images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        output_layer = F.relu(self.embedding(input).view(1, 1, -1))\n",
    "        output_layer, hidden_layer = self.gru(output_layer, hidden_layer)\n",
    "        output_layer = self.softmax(self.out(output_layer[0]))\n",
    "        return output_layer, hidden_layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "\n",
    "Without this, the context vector which is passed betweeen the encoder and decoder carries the burden of encoding the entire sentence.\n",
    "\n",
    "The attention RNN allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs.\n",
    "\n",
    "![diagram](images/attention-decoder.png)\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer, `attn`, using the decoder's input and hidden state as inputs.\n",
    "\n",
    "Rather than using [bucketing](https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing) for handling inputs of variable length in the training data, this example chooses a maximum sentence length that gets applied to all inputs: sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
    "\n",
    "![diagram](images/attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout, max_length):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input_layer, hidden_layer, encoder_output):\n",
    "        embedded = self.dropout(self.embedding(input_layer).view(1, 1, -1))\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden_layer[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_output.unsqueeze(0))\n",
    "        output_layer = self.attention_combine(torch.cat((embedded[0], attention_applied[0]), 1)).unsqueeze(0)\n",
    "        output_layer, hidden_layer = self.gru(F.relu(output_layer), hidden_layer)\n",
    "        output_layer = F.log_softmax(self.out(output_layer[0]), dim=1)\n",
    "        return output_layer, hidden_layer, attention_weights\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def in_minutes(seconds):\n",
    "    minutes = math.floor(seconds / 60)\n",
    "    seconds -= minutes * 60\n",
    "    return '%dm %ds' % (minutes, seconds)\n",
    "\n",
    "def since(prior, percent):\n",
    "    seconds = time.time() - prior\n",
    "    gap = (seconds / percent) - seconds\n",
    "    return '%s (- %s)' % (in_minutes(seconds), in_minutes(gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, enc_optimizer, dec_optimizer, crit, max_len):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_len, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        encoder_outputs[i] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for i in range(target_length):\n",
    "        if random.random() < teacher_forcing_ratio:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += crit(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            loss += crit(decoder_output, target_tensor[i])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensors(encoder, decoder, training_tensors, max_len, print_every, plot_every, learning_rate):\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    enc_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    dec_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    iterations = len(training_tensors)\n",
    "    for i in range(1, iterations+1):\n",
    "        training_pair = training_tensors[i-1]\n",
    "        input_tensor  = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, enc_optimizer, dec_optimizer, criterion, max_len)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (since(start, i/iterations), i, i/iterations * 100, print_loss_avg))\n",
    "            \n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    print('Preparing plot: %d losses (data points)' % len(losses))\n",
    "    show_plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Using the tab-delimited examples from [Anki](https://www.manythings.org/anki/) to see if it's possible to produce a simple English to Japanese translation service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: parsing Japanese text in python\n",
    "\n",
    "This proved to be surprisingly complicated. \n",
    "\n",
    "[KyTea](http://www.phontron.com/kytea/) would not install until going down to version `0.3.2`, and even then would not run properly.\n",
    "\n",
    "[This guide](images/japanese-spacy-and-mecab.pdf) (archived from the [original](https://www.dampfkraft.com/nlp/japanese-spacy-and-mecab.html)) was helpful, but I needed a few more tweaks to make it work:\n",
    "\n",
    "* `sudo ldconfig` gets around the `error while loading shared libraries: libmecab.so.2` building `unidic-mecab-2.1.2_src` (hat tip to [tatsuyaoiw](http://tatsuyaoiw.hatenablog.com/entry/20120414/1334397985))\n",
    "* edited `/usr/local/etc/mecabrc` so that `dicdir` is defined as `dicdir = /usr/local/lib/mecab/dic/unidic`\n",
    "* after `pip install mecab-python3==0.7` and `pip install spacy` I also needed `pip install fugashi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f ./data/jpn-eng.zip ]; then wget -P ./data https://www.manythings.org/anki/jpn-eng.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!if [ ! -f ./data/jpn.txt ]; then unzip -d ./data ./data/jpn-eng.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_file = Path.cwd() / \"data/jpn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, parser):\n",
    "        self.name = name\n",
    "        self.parser = parser\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in self.parser(sentence):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indices_from_sentence(self, sentence):\n",
    "        return [self.word2index[word] for word in self.parser(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Japanese language parser\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "ja = spacy.blank('ja')\n",
    "\n",
    "def parse_japanese(sentence):\n",
    "    return [word.text for word in ja(sentence)]\n",
    "\n",
    "\"\"\"\n",
    "English can be more complicated than this, \n",
    "but this suffices for this simple model\n",
    "\"\"\"\n",
    "\n",
    "def parse_english(sentence):\n",
    "    return sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_language_pairs(input_file=data_file):\n",
    "    pairs = [] # list of (eng, jpn) pairs\n",
    "    for line in data_file.read_text(encoding='utf-8').splitlines():\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) > 1:\n",
    "            pairs.append((parts[0].strip(), parts[1].strip()))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 20\n",
    "\n",
    "def filter_by_word_length(pair):\n",
    "    \"\"\"\n",
    "    Since the AttentionDecoderRNN is based on a max_length,\n",
    "    this function provides a filter based on that parameter \n",
    "    \"\"\"\n",
    "    return len(pair) == 2 \\\n",
    "      and len(parse_english(pair[0])) < MAX_WORD_LENGTH \\\n",
    "      and len(parse_japanese(pair[1])) < MAX_WORD_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [pair for pair in parse_language_pairs() if filter_by_word_length(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(parsed_pairs):\n",
    "    \"\"\"\n",
    "    Given a list of (eng, jpn) pairs, produce \n",
    "    a tuple (Lang(eng), Lang(jpn)) of the data\n",
    "    \"\"\"\n",
    "    eng = Lang('eng', parse_english)\n",
    "    jpn = Lang('jpn', parse_japanese)\n",
    "    for pair in parsed_pairs:\n",
    "        eng.add_sentence(pair[0])\n",
    "        jpn.add_sentence(pair[1])\n",
    "    return eng, jpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47612, 16361, 13291)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng, jpn = prepare_sample(samples)\n",
    "len(samples), eng.n_words, jpn.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting training data to [tensors](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor(lang, sentence):\n",
    "    indicies = lang.indices_from_sentence(sentence)\n",
    "    indicies.append(EOS_token)\n",
    "    return torch.tensor(indicies, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensors = [\n",
    "    (generate_tensor(eng, pair[0]), \n",
    "     generate_tensor(jpn, pair[1])) \n",
    "    for pair in samples\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "encoder_file = Path.cwd() / \"data/encoder.pkl\"\n",
    "decoder_file = Path.cwd() / \"data/decoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7m 30s (- 171m 21s) (2000 4%) 4.1404\n",
      "16m 28s (- 179m 41s) (4000 8%) 3.7958\n",
      "26m 11s (- 181m 39s) (6000 12%) 3.5731\n",
      "36m 23s (- 180m 9s) (8000 16%) 3.5235\n",
      "46m 57s (- 176m 35s) (10000 21%) 3.5602\n",
      "58m 0s (- 172m 8s) (12000 25%) 3.5699\n",
      "69m 38s (- 167m 12s) (14000 29%) 3.5354\n",
      "79m 56s (- 157m 56s) (16000 33%) 3.5719\n",
      "87m 17s (- 143m 36s) (18000 37%) 3.5440\n",
      "94m 50s (- 130m 55s) (20000 42%) 3.7074\n",
      "102m 38s (- 119m 29s) (22000 46%) 3.6637\n",
      "110m 35s (- 108m 48s) (24000 50%) 3.6851\n",
      "118m 45s (- 98m 43s) (26000 54%) 3.7732\n",
      "127m 8s (- 89m 3s) (28000 58%) 3.7161\n",
      "135m 36s (- 79m 36s) (30000 63%) 3.7713\n",
      "144m 20s (- 70m 25s) (32000 67%) 3.8579\n",
      "153m 15s (- 61m 21s) (34000 71%) 3.9100\n",
      "162m 28s (- 52m 24s) (36000 75%) 3.9732\n",
      "172m 5s (- 43m 31s) (38000 79%) 3.9219\n",
      "181m 54s (- 34m 36s) (40000 84%) 4.1058\n",
      "192m 7s (- 25m 40s) (42000 88%) 4.1527\n",
      "210m 10s (- 17m 15s) (44000 92%) 4.2791\n",
      "230m 16s (- 8m 4s) (46000 96%) 4.3954\n",
      "Preparing plot: 47 losses (data points)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "if encoder_file.exists() and decoder_file.exists():\n",
    "    encoder = pickle.load(open(encoder_file, 'rb'))\n",
    "    attn_decoder = pickle.load(open(decoder_file, 'rb'))\n",
    "else:\n",
    "    encoder = EncoderRNN(eng.n_words, hidden_size).to(device)\n",
    "    attn_decoder = AttentionDecoderRNN(hidden_size, jpn.n_words, 0.1, MAX_WORD_LENGTH).to(device)\n",
    "    train_tensors(encoder, attn_decoder, training_tensors, MAX_WORD_LENGTH, 2000, 1000, 0.02)\n",
    "    pickle.dump(encoder, open(encoder_file, 'wb'))\n",
    "    pickle.dump(attn_decoder, open(decoder_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, output_lang, max_length=MAX_WORD_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "        return decoded_words, decoder_attentions[:i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(encoder, decoder, pair, input_lang, output_lang):\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    output_words, attentions = evaluate(encoder, decoder,  generate_tensor(input_lang, pair[0]), output_lang)\n",
    "    output_sentence = u''.join(output_words)\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Everything will be fine.\n",
      "= 全部うまくいくよ。\n",
      "< 今日のはのはのののにのててててている。<EOS>\n",
      "\n",
      "> What he said turned out to be true.\n",
      "= 彼が言ったことは本当だとわかった。\n",
      "< 彼はにとととととととととととととた。<EOS>\n",
      "\n",
      "> I am a member of the basketball team.\n",
      "= ボクは、そのバスケットボールチームのメンバーだよ。\n",
      "< 私ののののののののをた。<EOS>\n",
      "\n",
      "> I have two brothers.\n",
      "= 私には２人兄弟がいます。\n",
      "< ２の２のののしていていた。<EOS>\n",
      "\n",
      "> Call me once you've arrived.\n",
      "= 着いたら電話してね。\n",
      "< 最近ののののののをををを。<EOS>\n",
      "\n",
      "> I had to walk because there were no taxis.\n",
      "= タクシーがいなかったので歩かなければいけなかった。\n",
      "< 昨日にににになかっなかった。<EOS>\n",
      "\n",
      "> He was condemned to death.\n",
      "= 彼は死刑を宣告された。\n",
      "< 彼は今ををににたた。<EOS>\n",
      "\n",
      "> I had no choice but to accept the offer.\n",
      "= その申し出を受け入れる他に選択肢がなかった。\n",
      "< 私はそのにににににににない。<EOS>\n",
      "\n",
      "> I don't want to talk to you.\n",
      "= あなたと話したくありません。\n",
      "< 話したくない人の話したくないと、話したくないと、ない。<EOS>\n",
      "\n",
      "> Please let me drive your new Toyota, too.\n",
      "= 君の新しいトヨタ、僕にも運転させて。\n",
      "< 新しいを新しいををお願いをお願いをください。<EOS>\n",
      "\n",
      "> Mother Teresa was born in Yugoslavia in 1910.\n",
      "= マザー・テレサは１９１０年にユーゴスラビアで生まれた。\n",
      "< 昨日のでととととたた。<EOS>\n",
      "\n",
      "> Tom thinks volleyball is more fun than basketball.\n",
      "= トムはバスケよりバレーの方が面白いと思っている。\n",
      "< トムはもっとよりより、より、より、をををををいる。<EOS>\n",
      "\n",
      "> He soon betrayed his ignorance.\n",
      "= 彼はすぐに無知をさらけ出した。\n",
      "< 彼はすぐのすぐのののをををををした。<EOS>\n",
      "\n",
      "> I have been to the airport to see him off.\n",
      "= 彼を見送りに空港に行ってきたところだ。\n",
      "< 私の駅のしてしている。<EOS>\n",
      "\n",
      "> The house was in flames.\n",
      "= 家は炎上していた。\n",
      "< 家の家の家の家のたたたた。<EOS>\n",
      "\n",
      "> He killed him.\n",
      "= 彼はその男を殺した。\n",
      "< 彼はをののののののををのをををををしした\n",
      "\n",
      "> Nowadays, traveling costs a lot of money.\n",
      "= この頃の旅行は金がかかる。\n",
      "< 多くのはのののののののたた。<EOS>\n",
      "\n",
      "> I am sure of her success.\n",
      "= 彼女はきっと成功する。\n",
      "< 彼女の成功のののののののののた。<EOS>\n",
      "\n",
      "> He readily agreed to my proposal.\n",
      "= 彼は私の申し出を快諾した。\n",
      "< 彼はののののののののををた。<EOS>\n",
      "\n",
      "> \"Does she play tennis?\" \"Yes, she does.\"\n",
      "= 「彼女はテニスをしますか」「はい、します」\n",
      "< 「彼女は、、、、、、ををををををた。<EOS>\n",
      "\n",
      "> Tom didn't know that Mary had decided to quit her job.\n",
      "= トムはメアリーが仕事を辞めると決めていたことを知らなかった。\n",
      "< トムはメアリーにメアリーににににに知られた。<EOS>\n",
      "\n",
      "> I hear that he's still alive.\n",
      "= 彼はまだ生きているそうだ。\n",
      "< まだまだのまだしたた。<EOS>\n",
      "\n",
      "> It was hot, so I turned on the fan.\n",
      "= 暑かったので扇風機をつけた。\n",
      "< 私のののののののののににににににた。<EOS>\n",
      "\n",
      "> The vending machines are over there.\n",
      "= 自動販売機はあそこにあります。\n",
      "< 隣ののののののののをををいる。<EOS>\n",
      "\n",
      "> Lend your money and lose your friend.\n",
      "= 金の貸し借り友誼の終わり。\n",
      "< お金お金さん、お金さんとお金をお金をお願いをお\n",
      "\n",
      "> In the summer, I go to the sea, and in the winter, to the mountains.\n",
      "= 夏は海に行き、冬は山に行きます。\n",
      "< 夏のののののののの行きます。<EOS>\n",
      "\n",
      "> I've decided to try doing that myself.\n",
      "= 自分でやってみることにしたよ。\n",
      "< 仕事のののののとととととないと思う。<EOS>\n",
      "\n",
      "> I can't do it.\n",
      "= 私にはできません。\n",
      "< もっとととととととないとないと、ない。<EOS>\n",
      "\n",
      "> Tom is extremely shy.\n",
      "= トムは人見知りが激しい。\n",
      "< トムはトムははのののいる。<EOS>\n",
      "\n",
      "> My daughter caught a cold.\n",
      "= うちの娘が風邪をひいた。\n",
      "< 風邪のののののののをををををををを。<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample some random entries\n",
    "for i in range(30):\n",
    "    pair = random.choice(samples)\n",
    "    evaluate_pair(encoder, attn_decoder, pair, eng, jpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
